{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53fda1e",
   "metadata": {},
   "source": [
    "# Computational graph example with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8040d062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\adria\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67eb72c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EINFACHES BERECHNUNGSGRAPH-BEISPIEL IN PYTORCH\n",
      "============================================================\n",
      "\n",
      "1. EINFACHES BEISPIEL:\n",
      "----------------------------------------\n",
      "x = 2.00\n",
      "y = 3.00\n",
      "z = x * y = 6.00\n",
      "w = z + x = 8.00\n",
      "loss = w² = 64.00\n",
      "\n",
      "Gradienten nach backward():\n",
      "∂loss/∂x = 64.00\n",
      "∂loss/∂y = 32.00\n",
      "Gradienten der Zwischentensoren:\n",
      "∂loss/∂z = 16.00\n",
      "∂loss/∂w = 16.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Automatischen Gradienten aktivieren (Standard in PyTorch)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EINFACHES BERECHNUNGSGRAPH-BEISPIEL IN PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Einfaches Beispiel: Grundlegende Operationen\n",
    "print(\"\\n1. EINFACHES BEISPIEL:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Eingabetensoren mit requires_grad=True für Gradientenberechnung\n",
    "# Leaf-Tensoren\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Vorwärtsberechnung (Forward Pass)\n",
    "# PyTorch baut automatisch den Berechnungsgraphen auf\n",
    "# Zwischentensoren\n",
    "z = x * y  # z = 2 * 3 = 6\n",
    "z.retain_grad() # Zwischentensoren müssen explizit retain_grad() aufrufen, um Gradienten zu speichern, weil sie keine Leaf-Tensoren sind und PyTorch standardmäßig nur Gradienten für Leaf-Tensoren speichert.\n",
    "w = z + x  # w = 6 + 2 = 8\n",
    "w.retain_grad()\n",
    "loss = w ** 2  # loss = 8^2 = 64\n",
    "\n",
    "print(f\"x = {x.item():.2f}\")\n",
    "print(f\"y = {y.item():.2f}\")\n",
    "print(f\"z = x * y = {z.item():.2f}\")\n",
    "print(f\"w = z + x = {w.item():.2f}\")\n",
    "print(f\"loss = w² = {loss.item():.2f}\")\n",
    "\n",
    "# Rückwärtsberechnung (Backward Pass)\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradienten nach backward():\")\n",
    "print(f\"∂loss/∂x = {x.grad.item():.2f}\")\n",
    "print(f\"∂loss/∂y = {y.grad.item():.2f}\")\n",
    "\n",
    "print(\"Gradienten der Zwischentensoren:\")\n",
    "print(f\"∂loss/∂z = {z.grad.item():.2f}\")\n",
    "print(f\"∂loss/∂w = {w.grad.item():.2f}\")\n",
    "\n",
    "# Manuelle Überprüfung der Gradienten:\n",
    "# f: R^2 --> R, (x,y) |-> loss\n",
    "# loss = (x*y + x)² = (2*3 + 2)² = 8² = 64\n",
    "# ∂loss/∂x = 2*(x*y + x) * (y + 1) = 2*8*4 = 64\n",
    "# ∂loss/∂y = 2*(x*y + x) * x = 2*8*2 = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8eff27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. NEURONALES NETZWERK BEISPIEL:\n",
      "----------------------------------------\n",
      "Eingabe: tensor([[1., 2.]])\n",
      "Netzwerk-Ausgabe: -0.7339\n",
      "Zielwert: 5.0000\n",
      "Verlust (MSE): 32.8778\n",
      "\n",
      "Gradienten der Netzwerk-Parameter:\n",
      "fc1.weight: Gradient-Shape = torch.Size([3, 2])\n",
      "  Gradient-Werte (erste 3): [0.0, 0.0, -4.587584495544434, -9.175168991088867, 5.762060165405273, 11.524120330810547]\n",
      "fc1.bias: Gradient-Shape = torch.Size([3])\n",
      "  Gradient-Werte (erste 3): [0.0, -4.587584495544434, 5.762060165405273]\n",
      "fc2.weight: Gradient-Shape = torch.Size([1, 3])\n",
      "  Gradient-Werte (erste 3): [0.0, -3.611833333969116, -19.162322998046875]\n",
      "fc2.bias: Gradient-Shape = torch.Size([1])\n",
      "  Gradient-Werte (erste 3): [-11.467826843261719]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. NEURONALES NETZWERK BEISPIEL:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Gradienten zurücksetzen\n",
    "x.grad = None\n",
    "y.grad = None\n",
    "\n",
    "# Ein einfaches neuronales Netzwerk definieren: 2->3->1 MLP\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)  # 2 Eingänge, 3 Neuronen\n",
    "        self.fc2 = nn.Linear(3, 1)  # 3 Eingänge, 1 Ausgang\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Netzwerk erstellen\n",
    "model = SimpleNet()\n",
    "\n",
    "# Eingabedaten (Batch-Größe 1, 2 Features)\n",
    "input_data = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "target = torch.tensor([[5.0]])\n",
    "\n",
    "# Forward Pass\n",
    "output = model(input_data)\n",
    "print(f\"Eingabe: {input_data.data}\")\n",
    "print(f\"Netzwerk-Ausgabe: {output.item():.4f}\")\n",
    "print(f\"Zielwert: {target.item():.4f}\")\n",
    "\n",
    "# Verlustfunktion\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(f\"Verlust (MSE): {loss.item():.4f}\")\n",
    "\n",
    "# Backward Pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradienten der Netzwerk-Parameter:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: Gradient-Shape = {param.grad.shape}\")\n",
    "        print(f\"  Gradient-Werte (erste 3): {param.grad.flatten().tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8145b6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.91112201"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5-0.1101)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db1364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. VISUALISIERUNG DES BERECHNUNGSGRAPHEN:\n",
      "----------------------------------------\n",
      "Berechnungsgraph-Struktur:\n",
      "       a(2.0)   b(3.0)\n",
      "         |   \\ /   |\n",
      "         |    X    |\n",
      "         |   / \\   |\n",
      "       c=a+b    d=a*b\n",
      "        (5)      (6)\n",
      "         \\      /\n",
      "          \\    /\n",
      "           e=c*d\n",
      "           (30)\n",
      "\n",
      "Werte:\n",
      "a = 2.0, b = 3.0\n",
      "c = a + b = 5.0\n",
      "d = a * b = 6.0\n",
      "e = c * d = 30.0\n",
      "\n",
      "Gradienten:\n",
      "∂e/∂a = 21.0\n",
      "∂e/∂b = 16.0\n",
      "\n",
      "============================================================\n",
      "WICHTIGE KONZEPTE:\n",
      "----------------------------------------\n",
      "1. requires_grad=True aktiviert die Gradientenverfolgung\n",
      "2. backward() berechnet alle Gradienten automatisch\n",
      "3. PyTorch erstellt den Berechnungsgraphen dynamisch\n",
      "4. Gradienten werden akkumuliert (addiert)\n",
      "5. Mit .grad kann auf die Gradienten zugegriffen werden\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. VISUALISIERUNG DES BERECHNUNGSGRAPHEN:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Neues einfaches Beispiel für bessere Visualisierung\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = a + b  # c = 5\n",
    "d = a * b  # d = 6\n",
    "e = c * d  # e = 30\n",
    "\n",
    "print(\"Berechnungsgraph-Struktur:\")\n",
    "print(\"       a(2.0)   b(3.0)\")\n",
    "print(\"         |   \\\\ /   |\")\n",
    "print(\"         |    X    |\")\n",
    "print(\"         |   / \\\\   |\")\n",
    "print(\"       c=a+b    d=a*b\")\n",
    "print(\"        (5)      (6)\")\n",
    "print(\"         \\\\      /\")\n",
    "print(\"          \\\\    /\")\n",
    "print(\"           e=c*d\")\n",
    "print(\"           (30)\")\n",
    "\n",
    "e.backward()\n",
    "print(f\"\\nWerte:\")\n",
    "print(f\"a = {a.item():.1f}, b = {b.item():.1f}\")\n",
    "print(f\"c = a + b = {c.item():.1f}\")\n",
    "print(f\"d = a * b = {d.item():.1f}\")\n",
    "print(f\"e = c * d = {e.item():.1f}\")\n",
    "\n",
    "print(f\"\\nGradienten:\")\n",
    "print(f\"∂e/∂a = {a.grad.item():.1f}\")  # ∂e/∂a = d + c = 6 + 5 = 11\n",
    "print(f\"∂e/∂b = {b.grad.item():.1f}\")  # ∂e/∂b = d + c = 6 + 5 = 11\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WICHTIGE KONZEPTE:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. requires_grad=True aktiviert die Gradientenverfolgung\")\n",
    "print(\"2. backward() berechnet alle Gradienten automatisch\")\n",
    "print(\"3. PyTorch erstellt den Berechnungsgraphen dynamisch\")\n",
    "print(\"4. Gradienten werden akkumuliert (addiert)\")\n",
    "print(\"5. Mit .grad kann auf die Gradienten zugegriffen werden\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebf1ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
