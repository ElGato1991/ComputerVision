{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209e4776",
   "metadata": {},
   "source": [
    "# Single Layer Perceptron (SLP) for MNIST Classification\n",
    "\n",
    "This notebook implements a simple Single Layer Perceptron to classify handwritten digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d9a1b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3a1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Softmax_fn = torch.nn.Softmax(dim=1)\n",
    "\n",
    "def show_data(data_tuple):\n",
    "    image, label = data_tuple\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e4cc92",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the training dataset:\\n\", train_dataset)\n",
    "\n",
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "print(\"Print the validating dataset:\\n \", validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21eeec5",
   "metadata": {},
   "source": [
    "## Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba53cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = plt.figure(figsize=(10,8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols*rows+1):\n",
    "    random_idx = torch.randint(len(train_dataset), size=(1,)).item() \n",
    "    img, label = train_dataset[random_idx]\n",
    "    h.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac43a6",
   "metadata": {},
   "source": [
    "## Define the SLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b02669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        pred = self.out(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c6f8b",
   "metadata": {},
   "source": [
    "## Initialize Model and Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 20  # can adjust e.g. 32 or 130\n",
    "output_dim = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device != \"cuda\":\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "print('The model: \\n', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fec2d9",
   "metadata": {},
   "source": [
    "## Inspect Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f042d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('W: ', list(model.parameters())[0].size())\n",
    "print('b: ', list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f76903",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hidden layer W: ', model.state_dict()['hidden.0.weight'].size())\n",
    "print('Hidden layer b: ', model.state_dict()['hidden.0.bias'].size())\n",
    "print('Output layer W: ', model.state_dict()['out.weight'].size())\n",
    "print('Output layer b: ', model.state_dict()['out.bias'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e0500",
   "metadata": {},
   "source": [
    "## Visualize Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc033c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotParameters(model, hiddenDim): \n",
    "    W = model.state_dict()['hidden.0.weight'].data.cpu()\n",
    "    b = model.state_dict()['hidden.0.bias'].data.cpu()\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    rows = int(np.ceil(hiddenDim/10.0))\n",
    "    fig, axes = plt.subplots(rows, 10, figsize=(20, rows*3))\n",
    "    fig.subplots_adjust(hspace=0.01, wspace=0.1)\n",
    "    axes_flat = axes.flat if hasattr(axes, 'flat') else [axes]\n",
    "    for i, ax in enumerate(axes_flat):\n",
    "        if i < hiddenDim:\n",
    "            ax.set_xlabel(f\"neuron: {i}\")\n",
    "            Img = W[i, :].view(28, 28)\n",
    "            ax.imshow(Img, vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "PlotParameters(model=model, hiddenDim=hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89298402",
   "metadata": {},
   "source": [
    "## Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Alternative:\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True, num_workers=1)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabfab2",
   "metadata": {},
   "source": [
    "## Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63837854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to track metrics\n",
    "train_accuracy_list = []\n",
    "loss_list = []\n",
    "\n",
    "# Training function\n",
    "\n",
    "def train(dataloader, model, loss_func, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batchNr, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(X.view(-1, input_dim))\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, yhat = torch.max(pred.data, 1)\n",
    "        correct += (yhat == y).sum().item()\n",
    "        train_loss += loss\n",
    "        if (batchNr+1) % 100 == 0:\n",
    "            loss_val, current = loss.item(), (batchNr+1)*len(y)\n",
    "            print(f\"loss: {loss_val:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "        # PlotParameters(model=model, hiddenDim=hidden_dim)\n",
    "\n",
    "    accuracy = correct / size\n",
    "    train_accuracy_list.append(accuracy)\n",
    "    loss_list.append(train_loss.item()/size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c976e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "\n",
    "# Validation function\n",
    "\n",
    "def validate(dataloader, model, loss_func):\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in dataloader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            pred = model(x_test.view(-1, input_dim))\n",
    "            val_loss += loss_func(pred, y_test)\n",
    "            _, yhat = torch.max(pred.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    val_accuracy_list.append(accuracy)\n",
    "    val_loss_list.append(val_loss.item())\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*accuracy):>0.1f} Avg loss: {(val_loss.item()):>8f}\")\n",
    "\n",
    "n_epochs = 10\n",
    "loss_list = []\n",
    "train_accuracy_list = []\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "N_train = len(train_dataset)\n",
    "N_test = len(validation_dataset)\n",
    "print(N_test)\n",
    "print(len(train_loader.dataset))\n",
    "\n",
    "for t in range(n_epochs):\n",
    "    print(f\"n_epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_func, optimizer)\n",
    "    validate(validation_loader, model, loss_func)\n",
    "print(\"Done!\")\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(loss_list, color=color)\n",
    "ax1.set_xlabel('epoch', color=color)\n",
    "ax1.set_ylabel('total loss', color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)\n",
    "ax2.plot(train_accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', color=color)\n",
    "fig.tight_layout()\n",
    "color = 'tab:orange'\n",
    "ax2.plot(val_accuracy_list, color=color)\n",
    "color = 'tab:purple'\n",
    "ax2.plot(val_loss_list, color=color)\n",
    "\n",
    "# Inference example\n",
    "X, y = validation_dataset[63]\n",
    "X = X.to(device)\n",
    "label_tensor = torch.tensor([y], device=device)\n",
    "z = model(X.reshape(-1, input_dim))\n",
    "_, yhat = torch.max(z, 1)\n",
    "yhat_int = int(yhat.item())\n",
    "X_cpu = X.cpu()\n",
    "show_data((X_cpu, y))\n",
    "plt.show()\n",
    "print(\"Predicted yhat:\", yhat_int)\n",
    "print(\"Raw logits z:\", z)\n",
    "print(\"Probability of predicted class\", torch.max(Softmax_fn(z)).item())\n",
    "print(\"Input reshaped:\", X_cpu.shape)\n",
    "hidden_out = model.hidden(X.reshape(-1, input_dim).to(device))\n",
    "print(\"Hidden layer output shape:\", hidden_out.shape)\n",
    "\n",
    "b = model.state_dict()['hidden.0.bias'].data.cpu()\n",
    "print(\"Bias sample (first 5):\", b[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
